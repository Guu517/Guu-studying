# QG训练营AI组第二周周记

2024.2.27



## 局部最优解

梯度是一个向量，每个元素为某点处函数的偏导数

**梯度下降法**是一种迭代方法，用来寻找函数的最小值点。它的思路是通过不断地调整参数的值，使函数在每一次迭代中向着最小值的方向移动。每次更新的幅度由一个称为**学习率**（步长）的参数控制

在迭代中梯度值不断变小

若样本数量非常多，使用**随机梯度下降(SGD)**，取少部分样本即每次从n个样本里面选择m个样本且样本不重复

#### 避免办法：

**动量随机梯度下降**在更新参数时，不仅考虑当前的梯度，还考虑上一次参数更新时的"动量"，避免陷入局部最优解

**自适应学习算法**自动调整学习率，如AdaGrad和RMSProp算法

**动量&自适应**学习：Adam算法

~我暂时不会用~



## 生活随记

黑眼圈越来越重了 : ) 

周末也睡得不怎么好最近，大组作业你…

三门数学确实上压力了最近！还是得要回归一点课内，防止绩点太难绷了hh。

六级啊六级啊六级啊！！！



